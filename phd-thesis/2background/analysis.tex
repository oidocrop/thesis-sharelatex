\section{Static Analysis}

The analysis of Android apps becomes more and more difficult currently. Both benign and malicious developers use various protection techniques, such as Java reflection, dynamic code loading and code obfuscation \cite{rastogi2013droidchameleon}, to prevent their apps from reverse-engineering. Java reflection and dynamic code loading techniques can dynamically launch specific behaviors, which can be only monitored in dynamic analysis environment instead of static analysis. Besides, in obfuscated apps, static analysis can only check the API-level behaviors of apps rather than the fine-grained behaviors, such as the URL in network connections and the phone number of sending SMS behavior.

Without above limitations of static analysis, the dynamic analysis approach is usually used for deeply analyzing apps.

\section{Bypassing Static Analysis via Dynamic Code Update}
%\subsection{Reflection-Bench and \\ InboxArchiver}
\label{sec:refbench_dcl}

This section demonstrates how malware developers can evade static analysis tools and the available online analysis systems using dynamic code updates. %Each feature, reflection and DCL, is  discussed separately. In the first subsection, we discuss Reflection-Bench (our benchmark of Android applications to test static analysis for reflection resolution), whereas in the second subsection, we discuss our sample test malware, InboxArchiver, which makes use of dynamic code loading to evade current available online analysis systems.

\iffalse
Problem Statement: Android has become the most widely used OS in mobile devices and so the number of apps available in Android markets has increased a great deal. Looking at the popularity of Android, adversaries are also attracted towards this platform and, hence, there has been a drastic increase in the number of malware. The recent mechanisms used in malware to avoid detection include reflection which is a feature of software engineering used to develop flexible apps. Reflection makes app analysis more challenging due to it's capability of modifying the app at runtime. 

Therefore, state-of-the-art static analysis tools do not provide sound results in the presence of reflection. Moreover, while there are benchmark applications available for testing other tools, there is a general lack of benchmark apps for testing Android based tools. More specifically, there is \ding{55} set of benchmark applications which use reflection to hide some malicious functionality, e.g., information leakage.  

The aim of this work is to explore the various ways in which reflection could be used to hide malicious functionalities; and incorporate these various ways to design/develop a set of benchmark Android applications which could be used to test the ability of static analysis tools in reflection resolution. As an initial work, we plan to analyze the state-of-the-art static analysis tools with these representative set of benchmark applications. \fi


\subsection{Reflection-Bench}
\label{refbench}

The usefulness of reflection in Android apps development is undoubted. However, reflection's inherent property to hinder static analysis of apps makes it attractive for malware developers. Although, researchers have worked on app analysis in the presence of reflection in Android apps, literature and the research community still lacks a benchmark of apps which could be used as a test suite to determine the effectiveness of app analysis tools in the presence of reflection. We present \emph{reflection-bench}, a set of Android apps, which use reflection to conceal information leakage to make detection harder for static anlyzers. Reflection-bench is designed so that it can be used to test tools which perform taint analysis as well as those that only generate call graphs for other forms of static analysis.

%, namely Flowdroid, IccTa, Androguard, Amandroid and Scandroid, and report the results here.


\iffalse
\begin{itemize}
  \item Model
  \todo[inline]{A basic high level picture}
  \item Design and Implementation
  \todo[inline]{Design and implementation details of individual apps in the benchmark}
  \item State-of-the-art Static analysis tools
  \todo[inline]{Overview of tools considered for analysis}
  \item Analysis results
  \todo[inline]{Results of the analysis tools on individual apps in the benchmark}
\end{itemize}
\fi

\textbf{Overview:} Reflection-bench consists of 14 apps which use reflection in various forms to conceal information leakage and make the flow of the program ambiguous. %Before describing the apps in reflection-bench, we go through the different cases of reflection that this benchmark covers. We divide the apps into different categories and try to make detection harder as we move from one case to the other. 
The hardness of resolving the targets of reflection depends upon the nature of the arguments used in the reflection APIs. We divide them into two classes,  i.e., statically availabe arguments (those string arguments which are provided as part of the app package, e.g., strings defined inside the program, read from a file which is part of the app, etc.) and statically unavailable arguments (those received over the network, read from files on disk, received from other apps, etc.).

Statically unavailable arguments can make it impossible for static analysis tools to resolve reflection. In reflection-bench, we only consider the case of statically available arguments. However, with each case the complexity is gradually increased. In the first few cases, the arguments of reflection APIs are constant strings assigned to program variables. In the latter cases, we consider reading the arguments from a properties file (part of the APK file) and from a hashtable defined inside the program. Moreover, we also consider the cases where the string arguments are formed from the concatenation of multiple strings or decrypted from encrypted strings using crypto APIs. In addition, we consider two levels of complexity where in level one, reflection is used to call only the methods defined inside the app and in level two, both the methods defined inside the program as well as the sensitive APIs, which are responsible for leaking sensitive information, are called through reflection.

\textbf{Implementation:} There are two major classes in each app, i.e, BaseClass and MainActivity. BaseClass has two methods, where \texttt{Method1} gets the device ID using the \texttt{getDeviceID} API and stores it in a local field \texttt{Str}. \texttt{Method2} gets a string and sends it out using the \texttt{sendTextMessage} API. MainActivity calls \texttt{Method1} of BaseClass, gets its field \texttt{Str} and sends it to the \texttt{Method2} of BaseClass which leaks it out. In the following, we describe how different combinations of reflection APIs are used in each case.

%We tried to cover different combinations of reflection APIs which could make it hard for static analyzers to detect the information leakage. In the following, we describe how reflection APIs are used in each case.

\iffalse
\begin{description}[style=unboxed,leftmargin=0cm]

\item \circled{1} MainActivity retrieves the  field \texttt{Str} of BaseClass using \texttt{getField} reflection API. %[Done-Tested]

\item \circled{2} MainActivity retrieves an instance of BaseClass using the reflection API \texttt{forName}, creates its object using the \texttt{newInstance} API and gets its field \texttt{Str} using the \texttt{getField} reflection API. %[Done-Tested]


\item \circled{3} MainActivity retrieves an instance of BaseClass using the reflection API \texttt{forName}, gets its Constructor using the \texttt{getConstructor} API, creates its object using the \texttt{newInstance} API and gets its field \texttt{Str} using the \texttt{getField} reflection API. %[Done-Tested]

\item \circled{4} MainActivity retrieves an instance of BaseClass using the reflection API \texttt{forName}, creates its object using the \texttt{newInstance} API and gets its field \texttt{Str} using the \texttt{getField} reflection API. It also retrieves the methods of BaseClass using the \texttt{getMethod} reflection API and calls them using the \texttt{invoke} reflection API. %[Done-Tested]

\item \circled{5} MainActivity retrieves an instance of BaseClass using the reflection API \texttt{forName}, gets its Constructor using the \texttt{getConstructor} API, creates its object using the \texttt{newInstance} API and gets its field \texttt{Str} using the \texttt{getField} reflection API. It also retrieves the methods of BaseClass using the \texttt{getMethod} reflection API and call them using the \texttt{invoke} reflection API. %[Done-Tested]

In the above cases, the names of the class "BaseClass", its methods and its field are provided as static strings in the MainActivity class. In the following, starting with Case \circled{4} as a base, we try to acquire/generate these names at runtime.

\item \circled{6} Reads the names of BaseClass, its methods and its field from a file. %[Done-Tested]

\item \circled{7} Reads the names of BaseClass, its methods and its field from a Hashtable.

\item \circled{8} Constructs the names of BaseClass, its methods and its field from multiple strings in the program.

\item \circled{9} Decrypts the encrypted names of BaseClass, its methods and its field using Crypto APIs.

In all of the above cases, reflection APIs are only used in MainActivity and the sensitive APIs, i.e., \texttt{getDeviceId} and \texttt{sendTextMessage}, are called directly in BaseClass. In the following cases, we introduce reflection in BaseClass too in addition to Case \circled{4}.

\item \circled{10} BaseClass retrieves  an instance of the TelephonyManager class using the reflection API \texttt{forName}, creates its object using the \texttt{newInstanc} API, gets the sensitive APIs using the \texttt{getMethod} reflection API and calls them using the \texttt{invoke} reflection API.

In the above case, we use static strings for the names of the class TelephonyManager and the methods \texttt{getDeviceId} and \texttt{sendTextMessage}. In the following we acquire/generate these names at runtime in addition to Case \circled{10}.

\item \circled{11} Reads the names of TelephonyManager class, methods \texttt{getDeviceId} and \texttt{sendTextMessage} from a file. 

\item \circled{12} Reads the names of TelephonyManager class, methods \texttt{getDeviceId} and \texttt{sendTextMessage} from a Hashtable. 

\item \circled{13} Constructs the names of TelephonyManager class, methods \texttt{getDeviceId} and \texttt{sendTextMessage} from multiple strings inside the app.

\item \circled{14} Decrypts the encrypted names of TelephonyManager class, methods \texttt{getDeviceId} and \texttt{sendTextMessage} using Crypto APIs.

\fi

\end{description}

\textbf{Tools analysis results:} We report the results of analysis on recent state-of-the-art tools, e.g., Flowdroid \cite{FlowDroid_Arzt2014}, Androguard \cite{AndroGuard_webpage}, Amandroid \cite{wei2014amandroid}, SAAF \cite{Saaf_Hoffmann2013}, SCandroid \cite{fuchs2009scandroid} and IccTa \cite{li2015iccta}. A summary of the results is provided in Table~\ref{tab:tools_comparison}. Those tools which perform taint analysis, such as Amandroid, etc., are analyzed by performing taint analysis of the apps in reflection-bench. However, for those tools which do not perform taint analysis, such as Androguard, etc., we analyze them by generating call graphs of the apps using these tools. In Table~\ref{tab:tools_comparison}, a \ding{51} in column X, indicates that the app is successfully analyzed by tool \textbf{X}, whereas, a \ding{55} indicates otherwise.

\iffalse
\textbf{Experiments}

We setup the mentioned tools on two machines. The reason for setting up these tools on two different systems is that some of these tools, such Flowdroid, SAAF, etc., require a lot of resources, which others such as Androguard and Stadyna do not require much resources. Therefore, tests with Androguard and Stadyna are performed on a machine with 2.5 GHz Intel Core i5 processor and 4 GB DDR3 memory. Since, Stadyna has a client side too, its client
is run on a Google Nexus S smartphone with the modified Android OS version 4.1.2 r2 connected to the server using a standard USB cable. The other tools are run on a machine having 8 processors, 1.80 GHz Intel (R) Xeon (R) E5-2603 each, and 8GB DDR3 memory. Although, the machines used are different and the tools used for analysis perform different forms of analysis, still, we present their performance later just to give an idea of their comparison.
\fi


%\textbf{Results}

%[COMMENT BRUNO: YOU NEED TO ADD THE EXPLANATION FOR N/A] 



\begin{table}[t]
%\centering
\caption{Analysis with State-of-the-art tools}
\label{tab:tools_comparison}

%\scriptsize
\centering

\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Apps}} &
\multicolumn{4}{c|}{\textbf{Taint Analysis}} & \multicolumn{3}{c|}{\textbf{Call Graphs}}\\
\cline{2-8}
& \rotatebox{90}{\textbf{Flowdroid}} & 
\rotatebox{90}{\textbf{IccTa}} &  \rotatebox{90}{\textbf{Amandroid}} & \rotatebox{90}{\textbf{SCandroid}} & \rotatebox{90}{\textbf{Androguard}} & \rotatebox{90}{\textbf{SAAF}} & \rotatebox{90}{\textbf{StaDART}} \\
\hline
\hline

\texttt{DataFlow1} & \ding{55} & \ding{55} & \ding{55} & - & NA & NA & NA\\
\hline
\texttt{PlainStringsL1-1} & \ding{55} & \ding{55} & \ding{55} & - & \ding{55} & \ding{55} & \ding{51} \\
\hline
\texttt{PlainStringsL1-2} & \ding{55} & \ding{55} & \ding{55} & - & \ding{55} & \ding{51} & \ding{51} \\
\hline
\texttt{PlainStringsL1-3} & \ding{55} & \ding{55} & \ding{55} & - & \ding{55} & \ding{51} & \ding{51}\\
\hline
\texttt{PlainStringsL1-4} & \ding{55} & \ding{55} & \ding{55} & - & \ding{55} & \ding{51} & \ding{51}\\
\hline
\texttt{FileStringsL1-1} & \ding{55} & \ding{55} & \ding{55} & - & \ding{55} & \ding{55} & \ding{51} \\
\hline
\texttt{HashtableStringsL1-1} & \ding{55} & \ding{55} & \ding{55} & - & \ding{55} & \ding{55} & \ding{51}\\
\hline
\texttt{MultipleStringsL1-1} & \ding{55} & \ding{55} & \ding{55} & - & \ding{55} & \ding{55} & \ding{51}\\
\hline
\texttt{EncryptedStringsL1-1} & \ding{55} & \ding{55} & \ding{55} & - & \ding{55} & \ding{55} & \ding{51}\\
\hline
\texttt{PlainStringsL2-1} & \ding{55} & \ding{55} & \ding{55} & - & \ding{55} & \ding{55} & \ding{51}\\
\hline
\texttt{FileStringsL2-1} & \ding{55} & \ding{55} & \ding{55} & - & \ding{55} & \ding{55} & \ding{51}\\
\hline
\texttt{HashtableStringsL2-1} & \ding{55} & \ding{55} & \ding{55} & - & \ding{55} & \ding{55} & \ding{51}\\
\hline
\texttt{MultipleStringsL2-1} & \ding{55} & \ding{55} & \ding{55} & - & \ding{55} & \ding{55} & \ding{51}\\
\hline
\texttt{EncryptedStringsL2-1} & \ding{55} & \ding{55} & \ding{55} & - & \ding{55} & \ding{55} & \ding{51}\\
\hline


\end{tabular}
\end{table}

\begin{description}[style=unboxed,leftmargin=0cm]


\item \textbf{Amandroid, Flowdroid, IccTa and SCandroid} To analyze reflection-bench with Amandroid, Flowdroid, IccTa and SCandroid, we performed taint analysis of the apps using these tools. They analyze APK files and report the presence of sources/sinks of information as well as the tainted paths between these sources and sinks, if any. As shown in Table~\ref{tab:tools_comparison}, Flowdroid did not report any information leakage in any of the apps. Although, it did report the presence of sources and sinks in some of the apps. Similar results are obtained with Amandroid and IccTa. None of these tools could detect the information flows obfuscated using reflection in reflection-Bench. With IccTa, it is understandably so, because it relies on Flowdroid for information flow analysis. SCanDroid terminated with an error without any meaningful results. This tool is not maintained anymore and no help was available to fix it. 

\iffalse

\textbf{Flowdroid}

1. Sources 4, sinks 2, paths none.
2. 0, 0, 0
3. 0, 0, 0
4. 0, 0, 0
5. 0, 0, 0
6. 0, 0, 0
7. 3, 1, 0
8. 0, 0, 0
9. 0, 0, 0
10. 0, 0, 0
11. 0, 0, 0
12. 0, 0, 0
13. 0, 0, 0
14. 0, 0, 0


\textbf{IccTa}

Based on Flowdroid and Epicc (now replaced with IC3).

\todo[inline]{Try the experiment again.}

\textbf{SCandroid}

\todo[inline]{Analysis ends with error!
Try again.}

\fi

\item \textbf{Androguard and SAAF} Since Androguard and SAAF generate method call graphs of apps, we analyze reflection-bench with these tools by generating the MCGs of the apps. In each of the generated MCGs, we look for the app's methods and APIs called through reflection. The first app of reflection-bench is only for those tools which perform taint analysis. It only uses reflection to make the data-flow ambiguous and does not effect the MCG. This is reflected by 'NA' (Not Applicable) in the first row of Table~\ref{tab:tools_comparison} for the tools that construct call graphs only. The rest of the apps can be used to test both kinds of tools. As shown in Table~\ref{tab:tools_comparison}, Androguard does not correctly identify any method called through reflection in any of the apps. SAAF's results are relatively better than Androguard's results. SAAF is able to correctly identify the targets of reflection calls in four of the apps in reflection-bench. In these four apps, the arguments provided to the reflection APIs are plain strings. SAAF does not resolve the targets in other cases where the arguments are either read from a file or hashtable, encrypted strings and formed from multiple strings inside the apps. It is important to remember here that none of the apps get any arguments from outside the app.


\item \textbf{StaDART} StaDART introduces the dynamic element in resolving the targets of reflection (further details are provided in \S4) and, therefore, it performs better as shown in Table~\ref{tab:tools_comparison}. It is based on Androguard and generates MCGs similar to those generated by Androguard. So, we do not analyze the first app in reflection-bench. Results of the rest of the apps, as shown by the \ding{51} in column 'StaDART', indicate that all the methods called through reflection are correctly identified by StaDART.



\end{description}

%[COMMENT BRUNO: REWRITE, TWEAKING IS NOT AN APPROPRIATE TERM. WHAT DO YOU MEAN?]

These analysis results show that reflection makes static analysis of apps harder. Specially, when the parameters of reflection APIs are not readily available in the code, static analysis tools find it extremely hard to properly analyze apps.  

\subsection{InboxArchiver: Test Malware using DCL}

App developers use dynamic code loading for various legitimate purposes, mainly extending the functionality of the app. However, this feature can be used by malware developers to bypass analysis tools deployed at the app markets. A malware developer can submit a seemingly benign app with hidden malicious functionality, i.e., obfuscated functionality to load additional code provided once the app is installed on a user's device. We demonstrate with our InboxArchiver app how a malware developer can bypass analysis tools using DCL.

\textbf{Overview:} InboxArchiver is a simple app that reads the SMS inbox and sends some statistics to a number provided by the user. These statistics include the number of SMS messages sent to and received from certain numbers. A user can configure InboxArchiver to receive a daily, weekly or monthly SMS message containing these statistics. The malicious part of the app, however, downloads some additional code from the Internet which contains other numbers potentially owned by an adversary, loads this code using the DCL APIs and leaks these SMS inbox statistics.

\textbf{Implementation:} The main features of InboxArchiver are the use of DCL and reflection having encrypted strings representing the code paths, class names and method names. This helps InboxArchiver to evade static analysis tools. In order to evade dynamic analysis, it makes use of a simple delay technique where again the APIs are called using reflection with encrypted parameters. It waits for 10 minutes before downloading the malicious code from the Internet and loading it using DCL. Although there are other more sophisticated anti-analysis techniques available, such as emulator detection, root detection, etc., the use of just a delay technique in InboxArchiver highlights the role of DCL/reflection in evading analysis tools.

\begin{wrapfigure}{l}{.24\textwidth}
%\centering
%\begin{center}
%\raggedleft
\includegraphics[scale=0.22]{figures/inboxarchiver.pdf}
%\end{center}
\caption{InboxArchiver}
\label{fig:inboxarchiver}
\end{wrapfigure}




 

%[COMMENT BRUNO: THIS SENTENCE SHOULD BE EXPLAINED BETTER. WHAT IS TEH DELAY]

InboxArchiver consists of three main classes, i.e., a \texttt{MainActivity} class, a \texttt{MessageSender} class and a \texttt{Loader} class. The \texttt{MainActivity} class presents an interface to the user as shown in Figure~\ref{fig:inboxarchiver}. The \texttt{MessageSender} class, which is a Service and runs in the background, is responsible for retrieving the inbox statistics and sending it periodically to a pre-configured number. After a certain delay, the \texttt{MessageSender} class instantiates an object of the \texttt{Loader} class which handles the downloading of additional code from the Internet and dynamically loading it using DCL APIs. It makes use of encrypted parameters and encryption/decryption functionality provided by other auxiliary classes. 

\textbf{Analysis results:} We uploaded InboxArchiver to a number of online Android app analysis systems. Table~\ref{tab:online-systems} shows a summary of the obtained results\footnote{Similar proof of concept apps, which were able to bypass the Google Bouncer check using dynamic code update features, can also be found in previous research~\cite{ExecuteThis_Poeplau2014,canfora2015composition}.}. Column \textit{Analyzed} shows whether the app is properly analyzed or not. The next two columns, \textit{Obfuscation} and \textit{DCL}, show if the analysis systems detect obfuscation and the use of dynamic code loading, respectively. The last column in the table represents the final remarks about the app. 

\begin{table}[]
\centering
\caption{InboxArchiver: Analysis Results}
\label{tab:online-systems}
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Analysis System} & \textbf{Analyzed} & \textbf{Obfuscation}  & \textbf{DCL} & \textbf{Malware}  \\ \hline
VirusTotal \cite{VirusTotal}        & \ding{51}  & \ding{55} & \ding{55}   & \ding{55} \\ \hline
UnDroid \cite{undroid}              & \ding{51}  & \ding{51} & \ding{55}   & \ding{55} \\ \hline
AndroTotal \cite{andrototal}        & \ding{51}  & \ding{55} & \ding{55}   & \ding{55} \\ \hline
ds-andrototal \cite{ds-andrototal}  & \ding{51}  & \ding{55} & \ding{55}   & \ding{55} \\ \hline
MobiSec Lab \cite{mobisec-lab}            & \ding{51}  & \ding{55} & \ding{55}   & \ding{55} \\ \hline
CopperDroid \cite{tam2015copperdroid}  & Queued     & -         & -           &  -        \\ \hline
SandDroid \cite{sanddroid}           & \ding{51}  & \ding{51} & \ding{51}   & \ding{55} \\ \hline
\end{tabular}
\end{table}

Among the online analysis tools shown in Table~\ref{tab:online-systems}, we did not receive any results from CopperDroid  and the app is still in the queue for more than a year now. All other tools were unable to detect that the submitted app is malicious. VirusTotal scanned the app with 54 antivirus tools, including BitDefender, GData, AVG, Avast and Kaspersky, etc., and none of them labeled it suspicious. UnDroid and SandDroid termed the app as obfuscated, while SandDroid could also detect dynamic code loading in the app. However, it could not detect the loaded file and analyze it.

%\cite{bitdefender_webpage}
%\cite{gdata_webpage}
%\cite{avg_webpage}
%\cite{avast_webpage}
%\cite{kaspersky_webpage}
%------------------------------------------------


\section{Dynamic analysis}


\cite{spreitzenbarth2013mobile}. Currently, dynamic analysis uses hooking techniques for monitoring behaviors of apps. The hooking techniques can be divided into two main types: 1) hooking Android framework APIs by modifying Android system \cite{zhauniarovich2015stadyna}\cite{enck2014taintdroid}, and 2) hooking APIs used in the app process by static instrumentation \cite{backes2013appguard}\cite{davis2013retroskeleton}. Both of them have limitations to analyze trick samples. The first hooking technique has a drawback that it cannot be used on other vendors' devices except for Google Nexus devices or emulators. This gives a chance for malicious apps, which uses the device fingerprint and anti-emulator technique to evade the dynamic detection. Even though the second hooking technique does not have this drawback, but it becomes useless when apps apply anti-repacking techniques. Apps can check the integrity of themselves in runtime and enter the frozen mode to prevent dynamic analysis if the integrity is broken. Moreover, if malicious apps implement malicious behaviors in native codes, the second hooking technique still cannot detect them.

Apps are analyzed for malicious contents before being published to the app markets. Many static and dynamic analysis techniques have been proposed for Android. The ded system re-targets Dalvik bytecode into Java class files that can be analyzed by the variety of tools developed for Java. %~\cite{Enck-USENIX-2011}. In the original paper,  the FortifySCA static analysis toolset was used for detecting vulnerabilities and dangerous functionality, like leaking the device IMEI.
DroidAlarm~\cite{Zhongyang-ASIACCS-2013} performs static detection of privilege-escalation vulnerabilities in apps by constructing paths in inter-procedural call graphs from a sensitive permission to a public interface accessible to other apps. Gascon et al. employ comparsion of functional call graphs (FCG) mined using AndroGuard to detetct malicious Android apps~\cite{Gascon-AISEC-2013}

Mobile apps are analyzed for malicious contents before being published to app stores, such as Google Play Store. The analysis usually involves two categories, \textit{i.e.,} static (reasoning about an app without executing it) and dynamic (executing apps in a controlled environment and understanding their behavior). Both of these analysis techniques have their pros and cons. While the former provides an over-approximation of what a piece of code actually performs, the latter misses certain execution paths due to limited duration of the analysis and the triggering problem. Static analysis also suffers from code obfuscation problems and dynamic code updates. Dynamic analysis, on the other hand, provides a solution to these problems, but requires test cases which could execute a major/required portion of the code.
Execution of certain code paths in mobile apps depends upon a combination of various user/system events. Generally, it is hard to predict inputs which can stimulate the required behavior in these apps. This feature of mobile apps is widely used by malware developers to conceal malicious functionality. 

Code coverage is a well-known limitation of dynamic analysis approaches. However, for the purpose of security analysis rather than testing, it is required to stimulate/reach only specific points of interest (POI) in the code rather than stimulating all the code paths in an app. In literature, researchers have focused mainly on providing inputs to make an app follow a specific path. Providing the exact inputs and environment becomes very hard as different apps may require different execution environments. Moreover, not all inputs can be predicted statically, because of obfuscation or other hiding techniques.

Static analysis relies on the availability of all the information at analysis time, hence, it suffers  from dynamic features and unavailability of information that are known only at execution time, e.g., the parameters used in the dynamic code update APIs. Therefore, reflection that is a programming technique widely used by mobile app developers can be only partially investigated by current  static analysis tools. As a result, reflection is usually used by malware developers to hide malicious code. The inherent limitation of  all static analyzers  (e.g., \cite{FlowDroid_Arzt2014,Saaf_Hoffmann2013}) is the operational assumption  that the code base does not change dynamically and the targets of reflection calls can be discovered in advance. This is a clear simplification of what happens in the real world, where many apps rely on code base updates instantiated only at runtime.

Dynamic analysis techniques are especially difficult to automate due to the need of emulating a comprehensive interactions of apps with the system and a user (UI interactions). Several approaches are proposed to automate the triggering of UI events, from random event generation~\cite{Hu-AST-2011} to more advanced approaches like AppsPlayground~\cite{AppsPlayground_Rastogi2013} and SmartDroid~\cite{SmartDroid_Zheng2012}. However, all of them still have many limitations on the type of events they can handle and the coverage.

\subsection{Runtime instrumentation}



